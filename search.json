[
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Below are useful links for machine learning and predictive modeling topics.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "references.html#machine-learning-packages",
    "href": "references.html#machine-learning-packages",
    "title": "References",
    "section": "Machine Learning Packages",
    "text": "Machine Learning Packages\n\nscikit-plot: package to build several plots not yet supported on sklearn like ks, cumulative gain and lift plots.\nfeature-engine: package for feature engineering within sklearn pipelines.\nXuniVerse: calculates information values (IV) and weights of evidence (woe)\nOptBinning: scorecard development\nPyCaret: automl package",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "references.html#machine-learning-courses",
    "href": "references.html#machine-learning-courses",
    "title": "References",
    "section": "Machine Learning Courses",
    "text": "Machine Learning Courses\n\nMachine Learning: Andrew Ng\nDTona - Canal Téo Me Why: aplicação de machine learning como ela é na vida real, saindo desde a construção da analytical base também até por o modelo em produção.\nKaggle Learn",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "references.html#books",
    "href": "references.html#books",
    "title": "References",
    "section": "Books",
    "text": "Books\n\nPython Data Science Handbook\nPython for Data Analysis\nStatistical Inference via Data Science",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "references.html#churn-and-propensity-modeling",
    "href": "references.html#churn-and-propensity-modeling",
    "title": "References",
    "section": "Churn and Propensity Modeling",
    "text": "Churn and Propensity Modeling\n\nPredict Customer Churn (the right way) using PyCaret\nDataiku Use Case: Churn Prediction\nPredicting Super Customers using Feature Labs and PyCaret\nComposeML: Predict Next Purchase",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "references.html#lead-scoring",
    "href": "references.html#lead-scoring",
    "title": "References",
    "section": "Lead Scoring",
    "text": "Lead Scoring\n\nLead Scoring: o guia definitivo para pontuar seus contatos automaticmaente\nA sofisticação do Lead Scoring com Data Science\nPredict Lead Score (the right way) using PyCaret",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "analytical_base_table.html",
    "href": "analytical_base_table.html",
    "title": "Analytical Base Table",
    "section": "",
    "text": "A Analytical Base Table (ABT), é a tabela analítica que é construída para treinar um modelo de machine learning. Cada registro deve representar uma entidade do problema que está sendo modelado e cada coluna uma feature da entidade, além de ter também uma coluna que será o target que queremos prever.\nAqui iremos construir uma analytical base table para prever churn. Para isso, iremos utilizar os dados da Olist disponíveis no Kaggle e iremos também construir a ABT utilizando Pandas. Caso queira uma outra discussão detalhada sobre construção de uma ABT de churn utilizando SQL ao invés de Pandas, recomendo fortemente um dos melhores conteúdos sobre machine learning na prática que existe, a playlista DTona no canal do youtube Téo Me Why.\nPrimeiro, vamos carregar as tabelas necessárias:\n\nimport dateutil\nimport numpy as np\nimport pandas as pd\n\n\ndf_order_items = pd.read_csv('../datasets/olist_order_items_dataset.csv')\ndf_orders = pd.read_csv('../datasets/olist_orders_dataset.csv', parse_dates=['order_approved_at'])\ndf_sellers = pd.read_csv('../datasets/olist_sellers_dataset.csv')\n\n\ndf_abt = pd.DataFrame()\n\nfor safra in pd.date_range('2017-03-01', '2018-03-01', freq='MS', normalize=True):\n    data_ref_safra = pd.to_datetime(safra).date()\n    data_inf_inclusiva = data_ref_safra - dateutil.relativedelta.relativedelta(months=12)\n    data_sup_exclusiva = data_ref_safra + dateutil.relativedelta.relativedelta(months=6)\n\n    # filtrando o período histórico\n    df_historico = (\n        df_order_items\n        .merge(df_orders, on='order_id', how='left')\n        .query(\" order_status == 'delivered' \")\n        .query(f\"order_approved_at &gt;= '{data_inf_inclusiva}' & order_approved_at &lt; '{data_sup_exclusiva}' \")\n        .merge(df_sellers, on='seller_id', how='left')\n    )  \n\n    # calculando as features\n    df_features = (\n        df_historico\n        .query(f'order_approved_at &lt; \"{data_ref_safra}\" ')\n        .groupby('seller_id')\n        .agg(\n            uf = ('seller_state', 'first'),\n            receita_12m = ('price', 'sum'),\n            qtde_orders_12m = ('order_id', 'nunique'),\n            qtde_items_12m = ('product_id', 'count'),\n            qtde_items_dist_12m = ('product_id', 'nunique'),\n            data_ult_vnd = ('order_approved_at', 'max')\n        )\n        .reset_index()\n        .assign(data_ref = pd.to_datetime(f'{data_ref_safra}'))\n        .assign(recencia = lambda df_: (df_['data_ref'] - df_['data_ult_vnd']).dt.days)\n    )     \n\n    # calculando o target\n    df_target = (\n        df_historico\n        .query(f'order_approved_at &gt;= \"{data_ref_safra}\" & order_approved_at &lt; \"{data_sup_exclusiva}\" ')\n        .filter(['seller_id'])\n        .drop_duplicates()\n    ) \n\n    # cruzando as features com o target: gerando a ABT\n    df_abt_safra = (\n        df_features\n        .merge(df_target, how='left', on='seller_id', indicator=True)\n        .assign(nao_revendeu_next_6m = lambda df_: np.where(df_['_merge'] == 'left_only', 1, 0))\n        .assign(data_ref = lambda df_: df_['data_ref'].dt.date)\n        .filter(['data_ref',\n                 'seller_id',\n                 'uf',\n                 'receita_12m',\n                 'qtde_orders_12m',\n                 'recencia',\n                 'qtde_items_12m',\n                 'qtde_items_dist_12m',\n                 'nao_revendeu_next_6m'\n        ])\n    )     \n\n    df_abt = pd.concat([df_abt, df_abt_safra])\n\n\nABT Construída\n\ndf_abt\n\n\n\n\n\n\n\n\n\ndata_ref\nseller_id\nuf\nreceita_12m\nqtde_orders_12m\nrecencia\nqtde_items_12m\nqtde_items_dist_12m\nnao_revendeu_next_6m\n\n\n\n\n0\n2017-03-01\n001cca7ae9ae17fb1caed9dfb1094831\nES\n899.10\n4\n0\n9\n1\n0\n\n\n1\n2017-03-01\n004c9cd9d87a3c30c522c48c4fc07416\nSP\n2629.31\n17\n1\n20\n15\n0\n\n\n2\n2017-03-01\n011b0eaba87386a2ae96a7d32bb531d1\nSP\n99.98\n1\n144\n2\n1\n1\n\n\n3\n2017-03-01\n014c0679dd340a0e338872e7ec85666a\nMG\n220.00\n2\n9\n2\n1\n0\n\n\n4\n2017-03-01\n01cf7e3d21494c41fb86034f2e714fa1\nPR\n992.90\n8\n4\n8\n3\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1869\n2018-03-01\nff82e8873fba613f2261a9acc896fd84\nMG\n124.60\n4\n12\n4\n3\n1\n\n\n1870\n2018-03-01\nffc470761de7d0232558ba5e786e57b7\nSP\n385.59\n5\n0\n5\n5\n0\n\n\n1871\n2018-03-01\nffdd9f82b9a447f6f8d4b91554cc7dd3\nPR\n1450.20\n11\n7\n12\n8\n0\n\n\n1872\n2018-03-01\nffeee66ac5d5a62fe688b9d26f83f534\nSP\n1709.87\n13\n0\n13\n3\n0\n\n\n1873\n2018-03-01\nfffd5413c0700ac820c7069d66d98c89\nSP\n5488.60\n33\n14\n34\n22\n0\n\n\n\n\n15656 rows × 9 columns\n\n\n\n\n\n\nPúblico e Churn no Tempo\nPara cada seller em um período de tempo (safra), temos uma série de features que irá nos ajudar a prever o target nao_revendeu_next_6m.\n\ndf_abt['data_ref'].value_counts(ascending=True).plot(kind='bar', title='Quantidade de Sellers por Safra', figsize=(10,5), rot=30);\n\n\n\n\n\n\n\n\n\ndf_abt.groupby('data_ref')['nao_revendeu_next_6m'].mean().plot(title='Percentual de Sellers que deram Churn por safra', figsize=(10,5), xlabel='');\n\n\n\n\n\n\n\n\n\n(df_abt.groupby(['data_ref', 'nao_revendeu_next_6m'])['nao_revendeu_next_6m'].size() / df_abt.groupby('data_ref').size()).unstack().plot(kind='area', figsize=(10,5), xlabel='');\n\n\n\n\n\n\n\n\nDe fato, o churn vem aumentando ao longo do tempo. Isso talvez seja esperado, dado que a operação da Olist em 2017-03 era bem recente e ali havíamos os primeiros clientes, chamados early adopters, que podem simplesmente ter testado o serviço e abandonado.\nComo próximo passo, podemos já treinar um modelo de machine learning. Mas esse é um assunto para o próximo post, fique ligado!\n\ndf_abt.to_csv(\"../datasets/abt_churn.csv\", index=False, sep=';')",
    "crumbs": [
      "Notebooks",
      "Analytical Base Table"
    ]
  },
  {
    "objectID": "did_minimum_wage.html",
    "href": "did_minimum_wage.html",
    "title": "Impact of minimum wage increase on employment rate of fast food restaurants",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\n\n\n# pull the data\ndataset = pd.read_csv(\"datasets/njmin3.csv\")\n\n\ndataset.head()\n\n\n\n\n\n\n\n\n\nNJ\nPOST_APRIL92\nNJ_POST_APRIL92\nfte\nbk\nkfc\nroys\nwendys\nco_owned\ncentralj\nsouthj\npa1\npa2\ndemp\n\n\n\n\n0\n1\n0\n0\n15.00\n1\n0\n0\n0\n0\n1\n0\n0\n0\n12.00\n\n\n1\n1\n0\n0\n15.00\n1\n0\n0\n0\n0\n1\n0\n0\n0\n6.50\n\n\n2\n1\n0\n0\n24.00\n0\n0\n1\n0\n0\n1\n0\n0\n0\n-1.00\n\n\n3\n1\n0\n0\n19.25\n0\n0\n1\n0\n1\n0\n0\n0\n0\n2.25\n\n\n4\n1\n0\n0\n21.50\n1\n0\n0\n0\n0\n0\n0\n0\n0\n13.00\n\n\n\n\n\n\n\n\n\nNJ: if the fast food restaurante is located at New Jersey (1) or Pensylvania (0)\nPOST_APRIL92: if the observation was recorded after (1) or before (0) april 92\nNJ_POST_APRIL92: multiplication of NJ by POST_APRIL92\nfte: full time employment rate\n\nEach line of the dataframe represents an observation of fte on a fast food restaurant.\n\ndataset.shape\n\n(820, 14)\n\n\n\ndataset.describe()\n\n\n\n\n\n\n\n\n\nNJ\nPOST_APRIL92\nNJ_POST_APRIL92\nfte\nbk\nkfc\nroys\nwendys\nco_owned\ncentralj\nsouthj\npa1\npa2\ndemp\n\n\n\n\ncount\n820.000000\n820.000000\n820.000000\n820.000000\n820.000000\n820.000000\n820.000000\n820.000000\n820.000000\n820.000000\n820.000000\n820.000000\n820.000000\n820.000000\n\n\nmean\n0.807317\n0.500000\n0.403659\n21.026511\n0.417073\n0.195122\n0.241463\n0.146341\n0.343902\n0.153659\n0.226829\n0.087805\n0.104878\n-0.070443\n\n\nstd\n0.394647\n0.500305\n0.490930\n9.271972\n0.493376\n0.396536\n0.428232\n0.353664\n0.475299\n0.360841\n0.419037\n0.283184\n0.306583\n8.725511\n\n\nmin\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n-41.500000\n\n\n25%\n1.000000\n0.000000\n0.000000\n15.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n-3.500000\n\n\n50%\n1.000000\n0.500000\n0.000000\n20.500000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n75%\n1.000000\n1.000000\n1.000000\n25.500000\n1.000000\n0.000000\n0.000000\n0.000000\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n4.000000\n\n\nmax\n1.000000\n1.000000\n1.000000\n85.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n34.000000\n\n\n\n\n\n\n\n\n\ndataset.isnull().sum()\n\nNJ                  0\nPOST_APRIL92        0\nNJ_POST_APRIL92     0\nfte                26\nbk                  0\nkfc                 0\nroys                0\nwendys              0\nco_owned            0\ncentralj            0\nsouthj              0\npa1                 0\npa2                 0\ndemp               52\ndtype: int64\n\n\n\n# replacing null values with averages\nfrom sklearn.impute import SimpleImputer\n\nmissingvalues_imputer = SimpleImputer(missing_values = np.nan, strategy = 'mean')\nmissingvalues_imputer.fit(dataset[['fte', 'demp']])\ndataset[['fte', 'demp']] = missingvalues_imputer.transform(dataset[['fte', 'demp']])\n\n\nDiD with Aggregated Metrics\n\ndataset.groupby(['NJ', 'POST_APRIL92'])['fte'].mean().reset_index()\n\n\n\n\n\n\n\n\n\nNJ\nPOST_APRIL92\nfte\n\n\n\n\n0\n0\n0\n23.272823\n\n\n1\n0\n1\n21.162064\n\n\n2\n1\n0\n20.457145\n\n\n3\n1\n1\n21.027396\n\n\n\n\n\n\n\n\n\n(NJ fte after treatment) - (NJ fte before treatment) = 21.03 - 20.46 = 0.57\n(PENN fte after treatment) - (PENN fte before treatment) = 21.162064 - 23.272823 = - 2.11\nDiD = 0.57 - (-2.11) = 0.57 + 2.11 = 2.68\nDiD = 2.68\n\nThe full time employment (fte) rate on New Jersey have an increase of 2.73 due to the minimum wage increase policy.\nIn other words, increasing the minimum wage has a positive impact on employment rate for fast food restaurants on New Jersey.\n\n\nDiD with Linear Regression\nLet NJ be represented by G and POST_APRIL92 represented by T. So the functional form of linear regression is:\n\\[fte(G,T) = \\beta_0 + \\beta_1 G + \\beta_2 T + \\beta_3 T G\\]\n\\[DiD = [fte(1,1) - fte(1,0)] - [fte(0,1) - fte(0,0)]\\]\n\\[DiD = [\\beta_0 + \\beta_1 + \\beta_2 + \\beta_3 - \\beta_0 - \\beta_1] - [\\beta_0 + \\beta_2 - \\beta_0]\\]\n\\[DiD = \\beta_2 + \\beta_3 - \\beta_2 = \\beta_3\\]\n\\[DiD = \\beta_3\\]\n\nX = dataset[['NJ', 'POST_APRIL92', 'NJ_POST_APRIL92']]\ny = dataset['fte'].values\n\n\nimport statsmodels.api as sm\nX = sm.add_constant(X)\nmodel1 = sm.OLS(y, X).fit()\n\n\nprint(model1.summary(yname=\"FTE\",\n                     xname=(\"intercept\", \"New Jersey\", \"After April 1992\", \"New Jersey and after April 1992\"),\n                     title=\"Model 1: FTE ~ NJ + POST_APRIL92 + NJ_POST_APRIL92\"))\n\n              Model 1: FTE ~ NJ + POST_APRIL92 + NJ_POST_APRIL92              \n==============================================================================\nDep. Variable:                    FTE   R-squared:                       0.007\nModel:                            OLS   Adj. R-squared:                  0.004\nMethod:                 Least Squares   F-statistic:                     1.974\nDate:                Wed, 28 Dec 2022   Prob (F-statistic):              0.116\nTime:                        20:11:03   Log-Likelihood:                -2986.2\nNo. Observations:                 820   AIC:                             5980.\nDf Residuals:                     816   BIC:                             5999.\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n===================================================================================================\n                                      coef    std err          t      P&gt;|t|      [0.025      0.975]\n---------------------------------------------------------------------------------------------------\nintercept                          23.2728      1.041     22.349      0.000      21.229      25.317\nNew Jersey                         -2.8157      1.159     -2.430      0.015      -5.091      -0.541\nAfter April 1992                   -2.1108      1.473     -1.433      0.152      -5.001       0.780\nNew Jersey and after April 1992     2.6810      1.639      1.636      0.102      -0.536       5.898\n==============================================================================\nOmnibus:                      232.659   Durbin-Watson:                   1.847\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              908.337\nSkew:                           1.289   Prob(JB):                    5.72e-198\nKurtosis:                       7.465   Cond. No.                         11.4\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nThe coefficient of the variable NJ_POST_APRIL92 = New Jersey and after April 1992 is 2.68, that is equal to the value founded by the aggregation method for DiD.\n\n\nReferences\n\nKaggle Notebook: Difference-in-Differences in Python\nIntuição do Diff-in-Diff no Blog Estatsite\nDoug McKee: An intuitive introduction to Difference-in-Differences\nCurso de Avaliação de Políticas Públicas do Prof. Felipe Nunes na UFMG\n\nEmenta e Material do curso\n\nMastering Econometrics with Joshua Angrist (MIT)",
    "crumbs": [
      "Econometry",
      "Impact of minimum wage increase on employment rate of fast food restaurants"
    ]
  },
  {
    "objectID": "00_about_me.html",
    "href": "00_about_me.html",
    "title": "About me",
    "section": "",
    "text": "Oi, eu sou João!\nSou bacharel em física, com mestrado e doutorando na área. Formado pela Universidade Federal do Ceará - UFC\nMeus tópicos de interesse acadêmico e pesquisa sempre estiveram relacionados com estatística, computação e claro, física.\nEm 2018 comecei a atuar como cientista de dados no mercado de trabalho, para um das maiores empresas de varejo e e-commerce do Brasil, a Via Varejo.\nHoje em dia atuo como Lead Data Scientist na Datarisk, uma consultoria especializada em modelagem preditiva e também dou aulas de Machine Learning e Python no Labdata, um departamento de tecnologia da Fundação Instituto de Administração focado em dados.",
    "crumbs": [
      "About me"
    ]
  },
  {
    "objectID": "00_about_me.html#about-me",
    "href": "00_about_me.html#about-me",
    "title": "About me",
    "section": "",
    "text": "Oi, eu sou João!\nSou bacharel em física, com mestrado e doutorando na área. Formado pela Universidade Federal do Ceará - UFC\nMeus tópicos de interesse acadêmico e pesquisa sempre estiveram relacionados com estatística, computação e claro, física.\nEm 2018 comecei a atuar como cientista de dados no mercado de trabalho, para um das maiores empresas de varejo e e-commerce do Brasil, a Via Varejo.\nHoje em dia atuo como Lead Data Scientist na Datarisk, uma consultoria especializada em modelagem preditiva e também dou aulas de Machine Learning e Python no Labdata, um departamento de tecnologia da Fundação Instituto de Administração focado em dados.",
    "crumbs": [
      "About me"
    ]
  },
  {
    "objectID": "monitoring.html",
    "href": "monitoring.html",
    "title": "Monitoramento",
    "section": "",
    "text": "import dateutil\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_validate, cross_val_predict\n\n\ndf_abt = pd.read_csv('../datasets/abt_churn.csv', sep=';')\n\ndf_train = df_abt.query(\"data_ref &lt; '2018-01-01'\")\ndf_oot = df_abt.query(\"data_ref &gt;= '2018-01-01'\")\n\n\ndf_train['data_ref'].value_counts()\n\n2017-12-01    1602\n2017-11-01    1415\n2017-10-01    1304\n2017-09-01    1181\n2017-08-01    1058\n2017-07-01     946\n2017-06-01     877\n2017-05-01     762\n2017-04-01     652\n2017-03-01     490\nName: data_ref, dtype: int64\n\n\n\ndf_oot['data_ref'].value_counts()\n\n2018-03-01    1874\n2018-02-01    1805\n2018-01-01    1690\nName: data_ref, dtype: int64\n\n\n\nX_train, y_train = df_train.drop(['data_ref', 'seller_id', 'nao_revendeu_next_6m'], axis=1), df_train['nao_revendeu_next_6m']\nX_oot, y_oot = df_oot.drop(['data_ref', 'seller_id', 'nao_revendeu_next_6m'], axis=1), df_oot['nao_revendeu_next_6m']\n\n\nfrom feature_engine.encoding import OneHotEncoder\n\nohe = OneHotEncoder(variables=['uf'])\nohe.fit(X_train)\nX_train = ohe.transform(X_train)\nX_oot = ohe.transform(X_oot)\n\n\nmodel = RandomForestClassifier(n_estimators=100, max_depth=3, random_state=42)\nauc_train = cross_validate(model, X_train, y_train, cv=5, scoring='roc_auc', n_jobs=-1)['test_score'].mean()\nmodel.fit(X_train, y_train)\nauc_oot = roc_auc_score(y_oot, model.predict_proba(X_oot)[:,1])\n\n\nauc_train, auc_oot\n\n(0.8732729818267317, 0.8911789919368343)\n\n\nVamos agora calcular a ROCAUC por safra:\n\ndf_abt['proba'] = model.predict_proba(pd.concat([X_train, X_oot]))[:,1]\ndf_abt.groupby('data_ref').apply(lambda df_: roc_auc_score(df_['nao_revendeu_next_6m'], df_['proba'])).plot(title='ROCAUC no tempo', figsize=(8,4), rot=30, xlabel='');\n\n\n\n\n\n\n\n\n\nTreinando uma RF para Prever se os Dados de Produção Mudaram\nTendo as bases de produção e treinamento, podemos criar um novo target que será igual a 1 para os dados de produção e 0 para os dados de treinamento. Se um modelo classificatório conseguir prever que tipo de dado pertence ao treino ou a produção (oot), dizemos que houve potencialmente um drift populacional, indicando provavelmente que um retreino do modelo deverá ser realizado ou pelo menos remover a variável(s) que mudou de comportamento.\n\nX_train['is_diff_from_train_data'] = 0\nX_oot['is_diff_from_train_data'] = 1\n\nabt_monitoramento = pd.concat([X_train, X_oot])\nX_monitoramento, y_monitoramento = abt_monitoramento.drop(['is_diff_from_train_data'], axis=1), abt_monitoramento['is_diff_from_train_data']\n\nmodel = RandomForestClassifier(n_estimators=100, max_depth=3, random_state=42)\nrocauc_cv_results = cross_validate(model, X_monitoramento, y_monitoramento, cv=5, scoring='roc_auc', n_jobs=-1)\nrocauc_cv_results['test_score'].mean(), rocauc_cv_results['test_score'].std()\n\n(0.6142722993625597, 0.055150121356693185)\n\n\nComo a ROCAUC deu acima de 0.50, de fato houve um drift populacional. Para saber qual variável mais impactou, basta que calculemos a feature importance.\n\nmodel.fit(X_monitoramento, y_monitoramento)\npd.Series(model.feature_importances_[:5], index=X_monitoramento.columns[:5]).sort_values(ascending=True).plot(kind='barh')\n\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(nrows=1, ncols=3, figsize=(24,4))\n\nfig.suptitle('Recencia, Receita e Quantidade de Pedidos mudam bastante ao longo do tempo')\ndf_abt.groupby('data_ref')['recencia'].mean().plot(ax=ax[0], rot=30, xlabel='', ylabel='Recencia')\ndf_abt.groupby('data_ref')['receita_12m'].mean().plot(ax=ax[1], rot=30, xlabel='', ylabel='Receita')\ndf_abt.groupby('data_ref')['qtde_orders_12m'].mean().plot(ax=ax[2], rot=30, xlabel='', ylabel='Quantidade de pedidos')\n\n\n\n\n\n\n\n\nEssas 3 variáveis estão mudando bastante, mas como estão mudando de forma crescente com o tempo, muito provavelmente o modelo já absorveu esse padrão e por isso a AUC entre treino e Out of Time se mantém em torno de 0.86\n\n\nUtilizando o PSI\nPopulation Stability Index\n\nX_train = X_train.drop('is_diff_from_train_data', axis=1)\nX_oot = X_oot.drop('is_diff_from_train_data', axis=1)\n\nmodel = RandomForestClassifier(n_estimators=100, max_depth=3, random_state=42)\n\ny_train_proba = cross_val_predict(model, X_train, y_train, cv=5, method='predict_proba', n_jobs=-1)[:,1]\nmodel.fit(X_train, y_train)\ny_oot_proba = model.predict_proba(X_oot)[:,1]\n\n\nfrom mletrics.stability import psi\n\npsi(y_train_proba, y_oot_proba)\n\n0.055625999560225715\n\n\nUm PSI de 0.05 indica que basicamente não houveram mudanças significativas na distribuição, como mostrado aqui.",
    "crumbs": [
      "Notebooks",
      "Monitoramento"
    ]
  },
  {
    "objectID": "model_probability_discretisation.html",
    "href": "model_probability_discretisation.html",
    "title": "Model Probability Discretisation",
    "section": "",
    "text": "We are going to discuss how to discretise (bin) the probabilites that our machine learning outputs.\nThis notebook is also published on the official feature-engine examples repo.\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_breast_cancer\n\n\nLoading the Dataset\n\nX, y = load_breast_cancer(return_X_y=True, as_frame=True)\nX.head(3)\n\n\n\n\n\n\n\n\n\nmean radius\nmean texture\nmean perimeter\nmean area\nmean smoothness\nmean compactness\nmean concavity\nmean concave points\nmean symmetry\nmean fractal dimension\n...\nworst radius\nworst texture\nworst perimeter\nworst area\nworst smoothness\nworst compactness\nworst concavity\nworst concave points\nworst symmetry\nworst fractal dimension\n\n\n\n\n0\n17.99\n10.38\n122.8\n1001.0\n0.11840\n0.27760\n0.3001\n0.14710\n0.2419\n0.07871\n...\n25.38\n17.33\n184.6\n2019.0\n0.1622\n0.6656\n0.7119\n0.2654\n0.4601\n0.11890\n\n\n1\n20.57\n17.77\n132.9\n1326.0\n0.08474\n0.07864\n0.0869\n0.07017\n0.1812\n0.05667\n...\n24.99\n23.41\n158.8\n1956.0\n0.1238\n0.1866\n0.2416\n0.1860\n0.2750\n0.08902\n\n\n2\n19.69\n21.25\n130.0\n1203.0\n0.10960\n0.15990\n0.1974\n0.12790\n0.2069\n0.05999\n...\n23.57\n25.53\n152.5\n1709.0\n0.1444\n0.4245\n0.4504\n0.2430\n0.3613\n0.08758\n\n\n\n\n3 rows × 30 columns\n\n\n\n\n\nnp.unique(y)\n\narray([0, 1])\n\n\n\nX.groupby(y).size()\n\ntarget\n0    212\n1    357\ndtype: int64\n\n\nWhen we want to build a model to rank, we would like to know if the mean of our target variable increases with the model predicted probability. In order to check that, it is common to discretise the model probabilities that is provided by model.predict_proba(X)[:, 1]. If the mean target increases monotonically with each bin boundaries, than we can rest assure that our model is doing some sort of ranking.\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.60, random_state=50)\n\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom feature_engine.wrappers import SklearnTransformerWrapper\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\n\nfeatures = X.columns.tolist()\n\nlr_model = Pipeline(steps=[\n    ('scaler', SklearnTransformerWrapper(transformer=MinMaxScaler(), variables=features)),\n    ('algorithm', LogisticRegression())\n])\n\n\nlr_model.fit(X_train, y_train)\n\ny_proba_train = lr_model.predict_proba(X_train)[:,1]\ny_proba_test = lr_model.predict_proba(X_test)[:,1]\n\n\nfrom sklearn.metrics import roc_auc_score\n\nprint(f\"Train ROCAUC: {roc_auc_score(y_train, y_proba_train):.4f}\")\nprint(f\"Test ROCAUC: {roc_auc_score(y_test, y_proba_test):.4f}\")\n\nTrain ROCAUC: 0.9972\nTest ROCAUC: 0.9892\n\n\nOur model is performing very good! But let’s check if it is in fact assigning the examples with the greatest chance to acquire breast cancer with higher probabilities.\nTo do that, let’s use the EqualFrequencyDiscretiser from feature-engine package.\nFirst, let’s build a dataframe with the predicted probabilities and the target.\n\npredictions_df = pd.DataFrame({'model_prob': y_proba_test,'target': y_test})\npredictions_df.head()\n\n\n\n\n\n\n\n\n\nmodel_prob\ntarget\n\n\n\n\n356\n0.782233\n1\n\n\n556\n0.993711\n1\n\n\n283\n0.136180\n0\n\n\n495\n0.792087\n1\n\n\n364\n0.951556\n1\n\n\n\n\n\n\n\n\n\nfrom feature_engine.discretisation import EqualFrequencyDiscretiser\n\ndisc = EqualFrequencyDiscretiser(q=4, variables=['model_prob'], return_boundaries=True)\npredictions_df_t = disc.fit_transform(predictions_df)\npredictions_df_t.groupby('model_prob')['target'].mean().plot(kind='bar', rot=45);\n\n\n\n\n\n\n\n\nIndeed our model is ranking! But the last two groups/bins with greater probabilities are really close to each other. So, maybe after all we just have 3 groups instead of 4. Wouldn’t it be nice if we could use a method that finds the optimum number of groups/bins for us? For this, feature-engine gotcha you! Let’s use the DecisionTreeDiscretiser. As said by Soledad Gali here, this discretisation technique consists of using a decision tree to identify the optimal partitions for a continuous variable, that is what our model probability is.\n\nfrom feature_engine.discretisation import DecisionTreeDiscretiser\n\ndisc = DecisionTreeDiscretiser(cv=3, scoring='roc_auc', variables=['model_prob'], regression=False)\n\npredictions_df_t = disc.fit_transform(predictions_df, y_test)\n\npredictions_df_t.groupby('model_prob')['target'].mean().plot(kind='bar', rot=0);\n\n\n\n\n\n\n\n\nVery nice! Our DecisionTreeDiscretiser have found three optimum bins/groups/clusters to split the model probability. The first group only contains examples that our model says won’t develop breast cancer. The second one has a 0.625 probability chance of develop a breast cancer and the third cluster has the greatest chance of develop breast cancer, 0.978.\nLet’ check the size of each cluster:\n\npredictions_df_t['model_prob'].value_counts().sort_index()\n\n0.000000     82\n0.625000      8\n0.978261    138\nName: model_prob, dtype: int64\n\n\nIt’s a common practice to give letters to each cluster in a way that the letter ‘A’ for example will be used to denote the cluster with less probability:\n\nimport string\n\ntree_predictions = np.sort(predictions_df_t['model_prob'].unique())\n\nratings_map = {tree_prediction: rating for rating, tree_prediction in zip(string.ascii_uppercase, tree_predictions)}\nratings_map\n\n{0.0: 'A', 0.625: 'B', 0.9782608695652174: 'C'}\n\n\n\npredictions_df_t['cluster'] = predictions_df_t['model_prob'].map(ratings_map)\npredictions_df_t.head()\n\n\n\n\n\n\n\n\n\nmodel_prob\ntarget\ncluster\n\n\n\n\n356\n0.978261\n1\nC\n\n\n556\n0.978261\n1\nC\n\n\n283\n0.000000\n0\nA\n\n\n495\n0.978261\n1\nC\n\n\n364\n0.978261\n1\nC\n\n\n\n\n\n\n\n\n\npredictions_df_t.groupby('cluster')['target'].mean().plot(kind='bar', rot=0, title=\"Mean Target by Cluster\");\n\n\n\n\n\n\n\n\nThe same figure as the one above, but now with letters to denote each cluster.\nTo finish, let’s see what are the boundaries of each cluster. With that information, once we apply the model to obtain the probability of develop breast cancer for a new sample, we can classify it into one of the three cluster we created with the DecisionTreeDiscretiser.\n\npredictions_df_t['model_probability'] = predictions_df['model_prob']\npredictions_df_t.head()\n\n\n\n\n\n\n\n\n\nmodel_prob\ntarget\ncluster\nmodel_probability\n\n\n\n\n356\n0.978261\n1\nC\n0.782233\n\n\n556\n0.978261\n1\nC\n0.993711\n\n\n283\n0.000000\n0\nA\n0.136180\n\n\n495\n0.978261\n1\nC\n0.792087\n\n\n364\n0.978261\n1\nC\n0.951556\n\n\n\n\n\n\n\n\n\npredictions_df_t.groupby('cluster').agg(lower_boundary = ('model_probability', 'min'), upper_boundary=('model_probability', 'max')).round(3)\n\n\n\n\n\n\n\n\n\nlower_boundary\nupper_boundary\n\n\ncluster\n\n\n\n\n\n\nA\n0.000\n0.467\n\n\nB\n0.473\n0.632\n\n\nC\n0.650\n0.998\n\n\n\n\n\n\n\n\nSo, if a new sample gets a probability of 0.72 it will be assigned to cluster C.\n\n\nReferences\nTo learn more about variable discretization tecniques, please go to https://trainindata.medium.com/variable-discretization-in-machine-learning-7b09009915c2.",
    "crumbs": [
      "Notebooks",
      "Model Probability Discretisation"
    ]
  },
  {
    "objectID": "cli.html",
    "href": "cli.html",
    "title": "cli",
    "section": "",
    "text": "The Param functions helps to initialize optional arguments to the cli (the ones we call with double dashes, like --to). If we don’t use Param and just initialize the parameters, it is going to be a positional (mandatory) argument.\n\nsource\n\nhello_blog\n\n hello_blog (to:str&lt;towhogiveshello&gt;='João')\n\nJust say hello\n\nsource\n\n\nbuild_lib\n\n build_lib ()\n\nBuild the lib and generate the .whl file"
  },
  {
    "objectID": "did_single_women_employment.html",
    "href": "did_single_women_employment.html",
    "title": "Impact of tax credit on single women employment",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\n\n\n# pull the data\ndataset = pd.read_stata(\"datasets/eitc.dta\")\n\n\ndataset.head()\n\n\n\n\n\n\n\n\n\nstate\nyear\nurate\nchildren\nnonwhite\nfinc\nearn\nage\ned\nwork\nunearn\n\n\n\n\n0\n11.0\n1991.0\n7.6\n0\n1\n18714.394273\n18714.394273\n26\n10\n1\n0.000000\n\n\n1\n12.0\n1991.0\n7.2\n1\n0\n4838.568282\n471.365639\n22\n9\n1\n4.367203\n\n\n2\n13.0\n1991.0\n6.4\n2\n0\n8178.193833\n0.000000\n33\n11\n0\n8.178194\n\n\n3\n14.0\n1991.0\n9.1\n0\n1\n9369.570485\n0.000000\n43\n11\n0\n9.369570\n\n\n4\n15.0\n1991.0\n8.6\n3\n1\n14706.607930\n14706.607930\n23\n7\n1\n0.000000\n\n\n\n\n\n\n\n\nEach row is an observation of a single woman.\n\ndataset.shape\n\n(13746, 11)\n\n\n\n# creating the modelling dummy variables\ndataset['is_mom'] = np.where(dataset['children'] &gt; 0, 1, 0)\ndataset['after93'] = np.where(dataset['year'] &gt; 1993, 1, 0)\ndataset['is_mom_after93'] = dataset['is_mom'] * dataset['after93']\n\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(7,4))\n\ndataset.groupby(['year', 'is_mom'])['work'].mean().unstack().plot(ax=ax)\nax.axvline(1993, color='k', linestyle='--', linewidth=1)\nax.set_xlabel('Year')\nax.set_ylabel('Work rate')\n# include a arrow that indicates points from text to the line\nax.annotate('Policy\\napplication', \n            xy=(1992.95, 0.5), \n            xytext=(1991.7, 0.53), \n            arrowprops={\"width\": 3, \"headwidth\": 10, \"color\": \"orange\"});\n\n\n\n\n\n\n\n\n\ndataset.head()\n\n\n\n\n\n\n\n\n\nstate\nyear\nurate\nchildren\nnonwhite\nfinc\nearn\nage\ned\nwork\nunearn\nis_mom\nafter93\nis_mon_after93\nis_mom_after93\n\n\n\n\n0\n11.0\n1991.0\n7.6\n0\n1\n18714.394273\n18714.394273\n26\n10\n1\n0.000000\n0\n0\n0\n0\n\n\n1\n12.0\n1991.0\n7.2\n1\n0\n4838.568282\n471.365639\n22\n9\n1\n4.367203\n1\n0\n0\n0\n\n\n2\n13.0\n1991.0\n6.4\n2\n0\n8178.193833\n0.000000\n33\n11\n0\n8.178194\n1\n0\n0\n0\n\n\n3\n14.0\n1991.0\n9.1\n0\n1\n9369.570485\n0.000000\n43\n11\n0\n9.369570\n0\n0\n0\n0\n\n\n4\n15.0\n1991.0\n8.6\n3\n1\n14706.607930\n14706.607930\n23\n7\n1\n0.000000\n1\n0\n0\n0\n\n\n\n\n\n\n\n\n\nDiD by aggregation\n\ndataset.groupby(['is_mom', 'after93'])['work'].mean().unstack()\n\n\n\n\n\n\n\n\nafter93\n0\n1\n\n\nis_mom\n\n\n\n\n\n\n0\n0.575460\n0.573386\n\n\n1\n0.445962\n0.490761\n\n\n\n\n\n\n\n\n\nwork(is mom after 93) - work(is_mom before 93) = 0.49 - 0.45 = 0.4\nwork(not mom after 93) - work(not mom before 93) = 0.57 - 0.58 = -0.1\nDiD = 0.4 - (-0.1) = 0.4 + 0.1 = 0.5\nDiD = 0.5\n\nThe employmet rate of mom’s that work has increased by 0.5 after the policy application.\n\n\nDiD by Logistic Regression\n\nX = dataset[['is_mom', 'after93', 'is_mom_after93']]\ny = dataset['work'].values\n\n\nimport statsmodels.api as sm\n\nX = sm.add_constant(X)\nmodel1 = sm.Logit(y, X).fit()\n\nOptimization terminated successfully.\n         Current function value: 0.686491\n         Iterations 4\n\n\n\nprint(model1.summary())\n\n                           Logit Regression Results                           \n==============================================================================\nDep. Variable:                      y   No. Observations:                13746\nModel:                          Logit   Df Residuals:                    13742\nMethod:                           MLE   Df Model:                            3\nDate:                Wed, 28 Dec 2022   Pseudo R-squ.:                0.009118\nTime:                        20:39:39   Log-Likelihood:                -9436.5\nconverged:                       True   LL-Null:                       -9523.3\nCovariance Type:            nonrobust   LLR p-value:                 2.058e-37\n==================================================================================\n                     coef    std err          z      P&gt;|z|      [0.025      0.975]\n----------------------------------------------------------------------------------\nconst              0.3042      0.036      8.443      0.000       0.234       0.375\nis_mom            -0.5212      0.047    -10.985      0.000      -0.614      -0.428\nafter93           -0.0085      0.053     -0.161      0.872      -0.112       0.095\nis_mom_after93     0.1885      0.070      2.708      0.007       0.052       0.325\n==================================================================================\n\n\nIt is not the same because it is a Logistic Regression!",
    "crumbs": [
      "Econometry",
      "Impact of tax credit on single women employment"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning Blog",
    "section": "",
    "text": "Nesse blog irei postar notebooks com exemplos de códigos sobre os mais diversos assuntos e tópicos dentro do universo de machine learning.",
    "crumbs": [
      "Machine Learning Blog"
    ]
  },
  {
    "objectID": "index.html#o-que-você-encontrará",
    "href": "index.html#o-que-você-encontrará",
    "title": "Machine Learning Blog",
    "section": "O que você encontrará",
    "text": "O que você encontrará\n\nNotebooks\nTextos\nRecomendações de cursos, tutoriais e livros",
    "crumbs": [
      "Machine Learning Blog"
    ]
  },
  {
    "objectID": "index.html#how-to-contribute",
    "href": "index.html#how-to-contribute",
    "title": "Machine Learning Blog",
    "section": "How to contribute",
    "text": "How to contribute\n\nClone this repo\nInstall nbdev with pip install nbdev\nRun nbdev_install_quarto\nRun nbdev_install_hooks\nMake the necessary changes (update the necessary files)\nRun nbdev_prepare\nRun nbdev_preview to check if everything is correctly updated\nCommit and push",
    "crumbs": [
      "Machine Learning Blog"
    ]
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "A Very Short Introduction to Machine Learning\nComo criar K-Fold Cross-Validation na mão em Python\nComo usar Pipelines no Scikit-Learn\nSpatio-Temporal Characteristics of Dengue Outbreaks\nBanking as a Service - Como a Datarisk pode ajudar a sua empresa a explorar o BaaS?\nUltrastrong plasmon-phonon coupling in double-layer graphene intercalated with a transition-metal dichalcogenide",
    "crumbs": [
      "Publications"
    ]
  }
]